# End-to-End DataOps Batch ETL Project

## 1. Project Overview
This project builds an **ETL pipeline** that transforms raw sales transaction data from **AWS S3** into an **Amazon Redshift** data warehouse for **business intelligence (BI) and reporting**. The pipeline follows a **star schema model** and is orchestrated using **Apache Airflow (local setup)**.

## 2. Problem Statement
E-commerce businesses store transactional data in **AWS S3**, but it remains **semi-structured and inefficient** for analysis. This project automates the process of extracting, transforming, and loading **new batch data** daily, enabling efficient sales, customer, and product analytics in **Redshift**.

## 3. Project Scope
### **Infrastructure (Terraform-Managed AWS Resources)**
‚úÖ **Amazon S3** ‚Äì Stores raw transaction data.  
‚úÖ **Amazon Redshift (Serverless)** ‚Äì Data warehouse for analytics.  
‚úÖ **IAM Roles & Policies** ‚Äì Secure S3-Redshift access.  
‚úÖ **Apache Airflow (Local Setup)** ‚Äì Orchestrates the ETL workflow.

### **ETL Process**
**1. Extract**: Copy new batch data from **S3** (Parquet format).  
**2. Transform**: Clean and structure data into **fact and dimension tables** using star schema approach.  
**3. Load**: Store structured data in **Amazon Redshift** for fast queries.  
**4. Orchestrate**: Automate ETL execution using **Airflow DAGs (local setup)**.

## 4. Tech Stack
- **Infrastructure as Code**: Terraform
- **Cloud Services**: AWS S3, Redshift, IAM
- **ETL Framework**: Python (Pandas/PySpark)
- **Orchestration**: Apache Airflow (Local Setup)
- **Storage Format**: Parquet

---
### üìå **Status:** In Progress ‚è≥  
A detailed breakdown of the ETL process and schema will be added upon completion

---
